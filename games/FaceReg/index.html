<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script>
<!DOCTYPE html>
<!-- saved from url=(0056)https://fiddle.jshell.net/affectiva/opyh5e8d/show/light/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <title>Emotion from Camera Sample App</title>
  
  <meta name="robots" content="noindex, nofollow">
  <meta name="googlebot" content="noindex, nofollow">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <script type="text/javascript" src="https://cdn.jsdelivr.net/gh/wxiguang/CDN-for-Blog/games/FaceReg/jquery-3.1.0.js"></script>

    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/wxiguang/CDN-for-Blog/games/FaceReg/result-light.css">

      <script type="text/javascript" src="https://cdn.jsdelivr.net/gh/wxiguang/CDN-for-Blog/games/FaceReg/bootstrap.min.js"></script>
      <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/wxiguang/CDN-for-Blog/games/FaceReg/bootstrap-theme.min.css">
      <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/wxiguang/CDN-for-Blog/games/FaceReg/bootstrap.min.css">
      <script type="text/javascript" src="https://cdn.jsdelivr.net/gh/wxiguang/CDN-for-Blog/games/FaceReg/affdex.js"></script>

  <style id="compiled-css" type="text/css">
      
  </style>


  
<meta name="generator" content="Hexo 4.2.0"></head>
<body>
    
  <div class="container-fluid">
    <div class="row">
      <div class="col-md-8" id="affdex_elements" style="width:680px;height:480px;"><video id="face_video" autoplay="" style="display: none;"></video><canvas id="face_video_canvas" width="640" height="480" style="display: block;"></canvas><script type="text/javascript" src="./adapter-1.4.0.js"></script></div>
      <div class="col-md-4">
        <div style="height:25em;">
          <strong>EMOTION TRACKING RESULTS</strong>
          <div id="results" style="word-wrap:break-word;"><span>Timestamp: 681.25</span><br><span>Number of faces found: 1</span><br><span>Appearance: {"gender":"Female","glasses":"No","age":"45 - 54","ethnicity":"Unknown"}</span><br><span>Emotions: {"joy":0,"sadness":0,"disgust":1,"contempt":0,"anger":0,"fear":0,"surprise":0,"valence":0,"engagement":0}</span><br><span>Expressions: {"smile":0,"innerBrowRaise":8,"browRaise":2,"browFurrow":0,"noseWrinkle":2,"upperLipRaise":0,"lipCornerDepressor":2,"chinRaise":0,"lipPucker":0,"lipPress":0,"lipSuck":0,"mouthOpen":0,"smirk":0,"eyeClosure":0,"attention":88,"lidTighten":0,"jawDrop":1,"dimpler":0,"eyeWiden":1,"cheekRaise":0,"lipStretch":0}</span><br><span>Emoji: üòê</span><br></div>
        </div>
        <div>
          <strong>DETECTOR LOG MSGS</strong>
        </div>
        <div id="logs"><span>Clicked the start button</span><br><span>Webcam access allowed</span><br><span>Clicked the reset button</span><br><span>Clicked the start button</span><br><span>The detector reports initialized</span><br></div>
      </div>
    </div>
    <div>
      <button id="start" onclick="onStart()">Start</button>
      <button id="stop" onclick="onStop()">Stop</button>
      <button id="reset" onclick="onReset()">Reset</button>
      <h3>Affectiva JS SDK CameraDetector to track different emotions.</h3>
      <p>
        <strong>Instructions</strong>
        <br>
        Press the start button to start the detector.
        <br> When a face is detected, the probabilities of the different emotions are written to the DOM.
        <br> Press the stop button to end the detector.
      </p>
    </div>
  </div>



  <!-- TODO: Missing CoffeeScript 2 -->

  <script type="text/javascript">//<![CDATA[

    
      // SDK Needs to create video and canvas nodes in the DOM in order to function
      // Here we are adding those nodes a predefined div.
      var divRoot = $("#affdex_elements")[0];
      var width = 640;
      var height = 480;
      var faceMode = affdex.FaceDetectorMode.LARGE_FACES;
      //Construct a CameraDetector and specify the image width / height and face detector mode.
      var detector = new affdex.CameraDetector(divRoot, width, height, faceMode);

      //Enable detection of all Expressions, Emotions and Emojis classifiers.
      detector.detectAllEmotions();
      detector.detectAllExpressions();
      detector.detectAllEmojis();
      detector.detectAllAppearance();

      //Add a callback to notify when the detector is initialized and ready for runing.
      detector.addEventListener("onInitializeSuccess", function() {
        log('#logs', "The detector reports initialized");
        //Display canvas instead of video feed because we want to draw the feature points on it
        $("#face_video_canvas").css("display", "block");
        $("#face_video").css("display", "none");
      });

      function log(node_name, msg) {
        $(node_name).append("<span>" + msg + "</span><br />")
      }

      //function executes when Start button is pushed.
      function onStart() {
        if (detector && !detector.isRunning) {
          $("#logs").html("");
          detector.start();
        }
        log('#logs', "Clicked the start button");
      }

      //function executes when the Stop button is pushed.
      function onStop() {
        log('#logs', "Clicked the stop button");
        if (detector && detector.isRunning) {
          detector.removeEventListener();
          detector.stop();
        }
      };

      //function executes when the Reset button is pushed.
      function onReset() {
        log('#logs', "Clicked the reset button");
        if (detector && detector.isRunning) {
          detector.reset();

          $('#results').html("");
        }
      };

      //Add a callback to notify when camera access is allowed
      detector.addEventListener("onWebcamConnectSuccess", function() {
        log('#logs', "Webcam access allowed");
      });

      //Add a callback to notify when camera access is denied
      detector.addEventListener("onWebcamConnectFailure", function() {
        log('#logs', "webcam denied");
        console.log("Webcam access denied");
      });

      //Add a callback to notify when detector is stopped
      detector.addEventListener("onStopSuccess", function() {
        log('#logs', "The detector reports stopped");
        $("#results").html("");
      });

      //Add a callback to receive the results from processing an image.
      //The faces object contains the list of the faces detected in an image.
      //Faces object contains probabilities for all the different expressions, emotions and appearance metrics
      detector.addEventListener("onImageResultsSuccess", function(faces, image, timestamp) {
        $('#results').html("");
        log('#results', "Timestamp: " + timestamp.toFixed(2));
        log('#results', "Number of faces found: " + faces.length);
        if (faces.length > 0) {
          log('#results', "Appearance: " + JSON.stringify(faces[0].appearance));
          log('#results', "Emotions: " + JSON.stringify(faces[0].emotions, function(key, val) {
            return val.toFixed ? Number(val.toFixed(0)) : val;
          }));
          log('#results', "Expressions: " + JSON.stringify(faces[0].expressions, function(key, val) {
            return val.toFixed ? Number(val.toFixed(0)) : val;
          }));
          log('#results', "Emoji: " + faces[0].emojis.dominantEmoji);
          if($('#face_video_canvas')[0] != null)
          	drawFeaturePoints(image, faces[0].featurePoints);
        }
      });

      //Draw the detected facial feature points on the image
      function drawFeaturePoints(img, featurePoints) {
        var contxt = $('#face_video_canvas')[0].getContext('2d');

        var hRatio = contxt.canvas.width / img.width;
        var vRatio = contxt.canvas.height / img.height;
        var ratio = Math.min(hRatio, vRatio);

        contxt.strokeStyle = "#FFFFFF";
        for (var id in featurePoints) {
          contxt.beginPath();
          contxt.arc(featurePoints[id].x,
            featurePoints[id].y, 2, 0, 2 * Math.PI);
          contxt.stroke();

        }
      }



  //]]></script>

  <script>
    // tell the embed parent frame the height of the content
    if (window.parent && window.parent.parent){
      window.parent.parent.postMessage(["resultsFrame", {
        height: document.body.getBoundingClientRect().height,
        slug: "opyh5e8d"
      }], "*")
    }

    // always overwrite window.name, in case users try to set it manually
    window.name = "result"
  </script>


</body></html>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>

